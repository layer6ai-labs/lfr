{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Length-of-stay Task Logistic Regression Model\n",
    "This notebook is with simplified dataloaders for logistic regression performance report.\n",
    "\n",
    "We use a pre-processed benchmark version of MIMIC-III. To produce this dataset please follow the instructions from https://github.com/ratschlab/ncl and https://github.com/YerevaNN/mimic3-benchmarks. We do not provide the original or preprocessed MIMIC-III dataset here and you must request for the access directly from https://mimic.physionet.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path ='../data/mimic3'\n",
    "from utils.load_data import load_mimic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, help='path to dataset or torch dataset name')\n",
    "parser.add_argument('-b', '--batch-size', default=512, type=int,\n",
    "                    metavar='N',\n",
    "                    help='mini-batch size (default: 512), this is the total '\n",
    "                         'batch size of all GPUs on the current node when '\n",
    "                         'using Data Parallel or Distributed Data Parallel')\n",
    "parser.add_argument('--method', type=str, default='lfr',  help='training method', \n",
    "                    choices=['lfr', 'supervised', 'supervised-aug', \n",
    "                             'autoencoder', 'simsiam', \n",
    "                             'simclr', 'diet', 'diet-aug'])\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 1)')\n",
    "parser.add_argument('--eval_bs', '--eval-batch-size', default=20480, type=int)\n",
    "\n",
    "\n",
    "args_string = ' --batch-size 20480 --dataset mimic3-los --method supervised'.split()\n",
    "args = parser.parse_args(args_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_mimic3(args, data_path, 'los', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(dataloader):\n",
    "    features, labels = [], []\n",
    "    for i, data in tqdm(enumerate(dataloader), total=len(dataloader), desc='loading data'):\n",
    "        x, y = data\n",
    "        features.extend(x.tolist())\n",
    "        labels.extend(y.tolist())\n",
    "    return features, labels\n",
    "\n",
    "x_train, y_train = get_data(train_loader)\n",
    "x_test, y_test = get_data(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2377738, 2016) (2377738,) (523200, 2016) (523200,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_eval import report_auprc, report_auroc, report_kappa\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if args.dataset == 'mimic3-decomp':\n",
    "    args.metrics = ['acc', 'auprc', 'auroc']\n",
    "elif args.dataset == 'mimic3-los':\n",
    "    args.metrics = ['acc', 'kappa']\n",
    "\n",
    "acc = []\n",
    "kappa = []\n",
    "auprc = []\n",
    "auroc = []\n",
    "for rand_seed in [0]:\n",
    "    np.random.seed(rand_seed)\n",
    "    print('start training')\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(x_train, y_train)\n",
    "    print('finished training')\n",
    "    predictions = lr.predict_proba(x_test)\n",
    "    predicted_labels = lr.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test, predicted_labels))\n",
    "    if 'kappa' in args.metrics:\n",
    "        kappa.append(report_kappa(y_test, predicted_labels))\n",
    "    if 'auprc' in args.metrics:\n",
    "        auprc.append(report_auprc(y_test, predictions[:, 1]))\n",
    "    if 'auroc' in args.metrics:\n",
    "        auroc.append(report_auroc(y_test, predictions[:, 1]))\n",
    "    print(acc, kappa, auprc, auroc)\n",
    "acc = np.array(acc)\n",
    "kappa = np.array(kappa)\n",
    "auprc = np.array(auprc)\n",
    "auroc = np.array(auroc)\n",
    "\n",
    "print('acc:', acc.mean(), acc.std())\n",
    "print('kappa:', kappa.mean(), kappa.std())\n",
    "print('auprc:', auprc.mean(), auprc.std())\n",
    "print('auroc:', auroc.mean(), auroc.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
